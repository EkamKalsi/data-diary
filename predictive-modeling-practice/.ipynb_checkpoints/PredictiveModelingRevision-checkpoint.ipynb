{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f348c69b-9b19-4cdf-80a6-39b657719eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf3aff22-03cb-435c-b97a-659b899b3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(row):\n",
    "    for name, value in row.items():\n",
    "        label = random.random()*value\n",
    "    return label + random.random()\n",
    "def generate_data(num_features, num_size):\n",
    "    np.random.seed(42)\n",
    "    cols = []\n",
    "    for i in range(1,num_features+1):\n",
    "        cols.append('X_' + str(i))\n",
    "    df = pd.DataFrame(np.random.rand(num_size, num_features), columns=cols)\n",
    "    df['label'] = df.apply(generate_label,axis=1)\n",
    "    return df\n",
    "all_df = generate_data(10,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ac27e50-422c-4a21-b2f4-52e979a4bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_data(df,label_column):\n",
    "    X = df.copy()\n",
    "    y = df[label_column]\n",
    "    X.drop(columns=label_column,inplace=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = split_data(all_df,'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db96b63-0c8a-455a-86b4-cec3d05059fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.0468079  -0.00069355  0.08657899 -0.0601108   0.0289736   0.0419658\n",
      " -0.10525025 -0.0255843   0.02071983  0.46226251]\n",
      "Intercept: 0.5050761857539199\n",
      "Feature Importance:\n",
      "   variable       imp\n",
      "0      X_1  0.046808\n",
      "1      X_2 -0.000694\n",
      "2      X_3  0.086579\n",
      "3      X_4 -0.060111\n",
      "4      X_5  0.028974\n",
      "5      X_6  0.041966\n",
      "6      X_7 -0.105250\n",
      "7      X_8 -0.025584\n",
      "8      X_9  0.020720\n",
      "9     X_10  0.462263\n",
      "R-squared: 0.17705053511035895\n",
      "RMSE:  0.33987572831404134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "def linear_regression(X_train, X_test, y_train, y_test):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "    \n",
    "    df_feature_imp = pd.DataFrame({'variable':X_train.columns, 'imp':model.coef_})\n",
    "    print('Feature Importance:\\n', df_feature_imp)\n",
    "\n",
    "    print('R-squared:', model.score(X_test, y_test))\n",
    "\n",
    "    model_predictions = model.predict(X_test)\n",
    "    print('RMSE: ', math.sqrt(mean_squared_error(y_test,model_predictions)))\n",
    "linear_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b0cc1de-6270-4c8d-81af-90e87f361a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.02145843 -0.00055304  0.02458198 -0.01761171  0.00836133  0.01226639\n",
      " -0.02973458 -0.00739245  0.0060569   0.13148112  0.03589811]\n",
      "Intercept: 0.747120880153343\n",
      "R-squared: 0.17783285071889143\n",
      "RMSE:  0.33971414287106855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def linear_regression_with_scaling(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "\n",
    "    \n",
    "    #df_feature_imp = pd.DataFrame({'variable':X_train.columns, 'imp':model.coef_})\n",
    "    #print('Feature Importance:\\n', df_feature_imp)\n",
    "\n",
    "    print('R-squared:', model.score(X_test, y_test))\n",
    "\n",
    "    model_predictions = model.predict(X_test)\n",
    "    print('RMSE: ', math.sqrt(mean_squared_error(y_test,model_predictions)))\n",
    "linear_regression_with_scaling(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87a089d-5a54-4bd9-a086-68c97262be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  label   R-squared:                       0.158\n",
      "Model:                            OLS   Adj. R-squared:                  0.147\n",
      "Method:                 Least Squares   F-statistic:                     14.81\n",
      "Date:                Thu, 27 Feb 2025   Prob (F-statistic):           2.26e-24\n",
      "Time:                        14:40:45   Log-Likelihood:                -243.81\n",
      "No. Observations:                 800   AIC:                             509.6\n",
      "Df Residuals:                     789   BIC:                             561.2\n",
      "Df Model:                          10                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5051      0.068      7.398      0.000       0.371       0.639\n",
      "X_1            0.0468      0.041      1.140      0.255      -0.034       0.127\n",
      "X_2           -0.0007      0.040     -0.017      0.986      -0.079       0.078\n",
      "X_3            0.0866      0.040      2.158      0.031       0.008       0.165\n",
      "X_4           -0.0601      0.041     -1.455      0.146      -0.141       0.021\n",
      "X_5            0.0290      0.041      0.707      0.480      -0.051       0.109\n",
      "X_6            0.0420      0.040      1.040      0.299      -0.037       0.121\n",
      "X_7           -0.1053      0.042     -2.532      0.012      -0.187      -0.024\n",
      "X_8           -0.0256      0.040     -0.632      0.527      -0.105       0.054\n",
      "X_9            0.0207      0.041      0.506      0.613      -0.060       0.101\n",
      "X_10           0.4623      0.041     11.157      0.000       0.381       0.544\n",
      "==============================================================================\n",
      "Omnibus:                       56.314   Durbin-Watson:                   1.949\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               19.960\n",
      "Skew:                           0.029   Prob(JB):                     4.63e-05\n",
      "Kurtosis:                       2.228   Cond. No.                         12.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "RMSE:  0.33987572831404095\n",
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                  label   R-squared (uncentered):                   0.828\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.827\n",
      "Method:                 Least Squares   F-statistic:                              955.6\n",
      "Date:                Thu, 27 Feb 2025   Prob (F-statistic):                    6.57e-78\n",
      "Time:                        14:40:45   Log-Likelihood:                         -67.667\n",
      "No. Observations:                 200   AIC:                                      137.3\n",
      "Df Residuals:                     199   BIC:                                      140.6\n",
      "Df Model:                           1                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.9762      0.032     30.912      0.000       0.914       1.038\n",
      "==============================================================================\n",
      "Omnibus:                        3.975   Durbin-Watson:                   1.555\n",
      "Prob(Omnibus):                  0.137   Jarque-Bera (JB):                2.511\n",
      "Skew:                          -0.006   Prob(JB):                        0.285\n",
      "Kurtosis:                       2.451   Cond. No.                         1.00\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] RÂ² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "def stats_linear_regression(X_train, X_test, y_train, y_test):\n",
    "    X_sm_train = sm.add_constant(X_train)\n",
    "    X_sm_test = sm.add_constant(X_test)\n",
    "    \n",
    "    model = sm.OLS(y_train, X_sm_train).fit()\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model_predictions = model.predict(X_sm_test)\n",
    "    print('RMSE: ', math.sqrt(mean_squared_error(y_test,model_predictions)))\n",
    "    print(sm.OLS(y_test, model_predictions).fit().summary())\n",
    "stats_linear_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cba9140-14cd-4867-943e-a8dab07e7c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for test: 0.17783285071889143\n",
      "Feature importance:     feat  importance\n",
      "0    X_1       -0.08\n",
      "1    X_2       -0.00\n",
      "2    X_3        0.08\n",
      "3    X_4       -0.06\n",
      "4    X_5        0.03\n",
      "5    X_6        0.04\n",
      "6    X_7       -0.10\n",
      "7    X_8       -0.03\n",
      "8    X_9        0.02\n",
      "9   X_10        0.46\n",
      "10  X_11        0.12\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  label   R-squared:                       0.159\n",
      "Model:                            OLS   Adj. R-squared:                  0.147\n",
      "Method:                 Least Squares   F-statistic:                     13.51\n",
      "Date:                Thu, 27 Feb 2025   Prob (F-statistic):           6.90e-24\n",
      "Time:                        18:57:37   Log-Likelihood:                -243.53\n",
      "No. Observations:                 800   AIC:                             511.1\n",
      "Df Residuals:                     788   BIC:                             567.3\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.5273      0.074      7.085      0.000       0.381       0.673\n",
      "X_1           -0.0751      0.167     -0.448      0.654      -0.404       0.254\n",
      "X_2           -0.0019      0.040     -0.047      0.963      -0.081       0.077\n",
      "X_3            0.0843      0.040      2.094      0.037       0.005       0.163\n",
      "X_4           -0.0617      0.041     -1.492      0.136      -0.143       0.019\n",
      "X_5            0.0292      0.041      0.713      0.476      -0.051       0.110\n",
      "X_6            0.0419      0.040      1.038      0.299      -0.037       0.121\n",
      "X_7           -0.1048      0.042     -2.521      0.012      -0.186      -0.023\n",
      "X_8           -0.0253      0.040     -0.625      0.532      -0.105       0.054\n",
      "X_9            0.0210      0.041      0.513      0.608      -0.059       0.101\n",
      "X_10           0.4624      0.041     11.158      0.000       0.381       0.544\n",
      "X_11           0.1223      0.163      0.751      0.453      -0.197       0.442\n",
      "==============================================================================\n",
      "Omnibus:                       54.408   Durbin-Watson:                   1.953\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               19.568\n",
      "Skew:                           0.026   Prob(JB):                     5.64e-05\n",
      "Kurtosis:                       2.236   Cond. No.                         38.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "def quadratic_regression(X_train, X_test, y_train, y_test):\n",
    "    # introducing quadratic variable\n",
    "    X_train['X_11'] = X_train['X_1']*X_train['X_1']\n",
    "    X_test['X_11'] = X_test['X_1']*X_test['X_1']\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feat': X_train.columns,\n",
    "        'importance':model.coef_\n",
    "    })\n",
    "    feat_importance['importance'] = feat_importance['importance'].round(2)\n",
    "    print('R-squared for test:', model.score(X_test, y_test))\n",
    "    print('Feature importance:', feat_importance)\n",
    "    \n",
    "\n",
    "    model_stat = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "\n",
    "    print(model_stat.summary())\n",
    "quadratic_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88276410-09b6-4a33-8499-c88afd558c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared for test: 0.17805030687298695\n",
      "Feature importance:      feat  importance\n",
      "0     X_1       -0.07\n",
      "1     X_2       -0.00\n",
      "2     X_3        0.08\n",
      "3     X_4       -0.06\n",
      "4     X_5        0.03\n",
      "5     X_6        0.04\n",
      "6     X_7       -0.10\n",
      "7     X_8       -0.02\n",
      "8     X_9        0.02\n",
      "9    X_10        0.46\n",
      "10   X_11        0.12\n",
      "11  X_cat        0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "def one_hot_encoding(X_train, X_test, y_train, y_test,encoding_type='label'):\n",
    "    random_str = random.choices('ABC', k=len(X_train))\n",
    "    X_train['X_cat'] = random_str\n",
    "\n",
    "    random_str = random.choices('ABC', k=len(X_test))\n",
    "    X_test['X_cat'] = random_str\n",
    "\n",
    "    if encoding_type=='label':\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(X_train['X_cat'])\n",
    "        X_train_cat = X_train.copy()\n",
    "        X_test_cat = X_test.copy()\n",
    "        X_train_cat['X_cat'] = encoder.transform(X_train['X_cat'])\n",
    "        X_test_cat['X_cat'] = encoder.transform(X_test['X_cat'])\n",
    "        \n",
    "\n",
    "    elif encoding_type=='one_hot':\n",
    "        X_train_cat = pd.get_dummies(X_train,columns=['X_cat'],drop_first=True)\n",
    "        X_test_cat = pd.get_dummies(X_test,columns=['X_cat'],drop_first=True)\n",
    "    else:\n",
    "        print('Adjust Label')\n",
    "        return\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_cat,y_train)\n",
    "\n",
    "\n",
    "    feat_importance = pd.DataFrame({\n",
    "        'feat': X_train_cat.columns,\n",
    "        'importance':model.coef_\n",
    "    })\n",
    "    feat_importance['importance'] = feat_importance['importance'].round(2)\n",
    "    print('R-squared for test:', model.score(X_test_cat, y_test))\n",
    "    print('Feature importance:', feat_importance)\n",
    "one_hot_encoding(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f26c8e91-1253-4b8b-9992-d1bb92012719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [ 0.00163394 -0.00140457 -0.00584777 -0.01820991 -0.0034804   0.00274065\n",
      "  0.01434933 -0.00868348 -0.00632897  0.13429372]\n",
      "Intercept: 0.7463991375461476\n",
      "R-squared: 0.15568745583078591\n",
      "RMSE:  0.3247540582643224\n",
      "Best alpha for RidgeCV is: 46.41588833612773\n",
      "R-squared: 0.15625725146064018\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 34\u001b[0m\n\u001b[1;32m     29\u001b[0m     feature_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m:X_train\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m:model_alpha\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m     32\u001b[0m     })\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(feature_imp)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mridge_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[47], line 30\u001b[0m, in \u001b[0;36mridge_regression\u001b[0;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m     26\u001b[0m model_alpha\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR-squared:\u001b[39m\u001b[38;5;124m'\u001b[39m, model_alpha\u001b[38;5;241m.\u001b[39mscore(X_test, y_test))\n\u001b[1;32m     29\u001b[0m feature_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimportance\u001b[39m\u001b[38;5;124m'\u001b[39m:model_alpha\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m     32\u001b[0m })\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_imp)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "def ridge_regression(X_train, X_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    column_names = X_train.columns\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    model = Ridge()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "    print('R-squared:', model.score(X_test, y_test))\n",
    "    model_predictions = model.predict(X_test)\n",
    "    print('RMSE: ', math.sqrt(mean_squared_error(y_test,model_predictions)))\n",
    "\n",
    "    lambdas = np.logspace(-3,3,10)\n",
    "    model_cv = RidgeCV(alphas=lambdas, cv=5)\n",
    "    model_cv.fit(X_train, y_train)\n",
    "\n",
    "    print('Best alpha for RidgeCV is:', model_cv.alpha_)\n",
    "\n",
    "    model_alpha = Ridge(alpha=model_cv.alpha_)\n",
    "    model_alpha.fit(X_train, y_train)\n",
    "    print('R-squared:', model_alpha.score(X_test, y_test))\n",
    "\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature':X_train.columns,\n",
    "        'importance':model_alpha.coef_\n",
    "    })\n",
    "    print(feature_imp)\n",
    "ridge_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73019008-491c-416e-9822-d52e658866ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
